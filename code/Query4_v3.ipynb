{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2eed445-da6f-42da-bc9b-060f99a8bb4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.memory': '8g', 'spark.executor.cores': '4'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>847</td><td>application_1761923966900_0859</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0859/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-189.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0859_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>848</td><td>application_1761923966900_0860</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0860/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-97.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0860_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>849</td><td>application_1761923966900_0861</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0861/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-160.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0861_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>850</td><td>application_1761923966900_0862</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0862/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-189.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0862_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>851</td><td>application_1761923966900_0863</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0863/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-189.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0863_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>856</td><td>application_1761923966900_0868</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0868/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-172.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0868_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>857</td><td>application_1761923966900_0869</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0869/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-175.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0869_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>858</td><td>application_1761923966900_0870</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0870/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-133.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0870_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"8g\",\n",
    "        \"spark.executor.cores\": \"4\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf913d0-9fc5-46cb-a81c-66cc8e0871cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6174392acb91446b96e8544229aa706f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+--------------------+\n",
      "|division        |crime_count|avg_distance        |\n",
      "+----------------+-----------+--------------------+\n",
      "|HOLLYWOOD       |214157     |0.02043621355704596 |\n",
      "|VAN NUYS        |212495     |0.028654072511946814|\n",
      "|WILSHIRE        |199273     |0.026311034884955453|\n",
      "|SOUTHWEST       |187418     |0.0215807431211962  |\n",
      "|OLYMPIC         |181990     |0.017306293320735553|\n",
      "|NORTH HOLLYWOOD |171974     |0.0261161022174779  |\n",
      "|77TH STREET     |168030     |0.016587006268602513|\n",
      "|PACIFIC         |158587     |0.037524534857775786|\n",
      "|CENTRAL         |155553     |0.009875953835326358|\n",
      "|SOUTHEAST       |153569     |0.024162560852895597|\n",
      "|RAMPART         |150615     |0.014739396233047729|\n",
      "|TOPANGA         |150239     |0.03244665466643062 |\n",
      "|WEST VALLEY     |132004     |0.028994301945575132|\n",
      "|HARBOR          |130520     |3.000764735901569   |\n",
      "|FOOTHILL        |122935     |0.04126626556458656 |\n",
      "|WEST LOS ANGELES|121644     |0.02981869988903158 |\n",
      "|HOLLENBECK      |120059     |0.026379006753597365|\n",
      "|MISSION         |111032     |0.03498260536785933 |\n",
      "|NEWTON          |109570     |0.0158943039298326  |\n",
      "|NORTHEAST       |106135     |0.03906315361123133 |\n",
      "+----------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (41)\n",
      "+- Sort (40)\n",
      "   +- Exchange (39)\n",
      "      +- HashAggregate (38)\n",
      "         +- Exchange (37)\n",
      "            +- HashAggregate (36)\n",
      "               +- Project (35)\n",
      "                  +- SortMergeJoin Inner (34)\n",
      "                     :- Sort (15)\n",
      "                     :  +- Exchange (14)\n",
      "                     :     +- Filter (13)\n",
      "                     :        +- Project (12)\n",
      "                     :           +- BroadcastNestedLoopJoin Cross BuildRight (11)\n",
      "                     :              :- Union (7)\n",
      "                     :              :  :- Project (3)\n",
      "                     :              :  :  +- Filter (2)\n",
      "                     :              :  :     +- Scan csv  (1)\n",
      "                     :              :  +- Project (6)\n",
      "                     :              :     +- Filter (5)\n",
      "                     :              :        +- Scan csv  (4)\n",
      "                     :              +- BroadcastExchange (10)\n",
      "                     :                 +- Project (9)\n",
      "                     :                    +- Scan csv  (8)\n",
      "                     +- Sort (33)\n",
      "                        +- Exchange (32)\n",
      "                           +- Filter (31)\n",
      "                              +- HashAggregate (30)\n",
      "                                 +- Exchange (29)\n",
      "                                    +- HashAggregate (28)\n",
      "                                       +- Project (27)\n",
      "                                          +- BroadcastNestedLoopJoin Cross BuildRight (26)\n",
      "                                             :- Union (22)\n",
      "                                             :  :- Project (18)\n",
      "                                             :  :  +- Filter (17)\n",
      "                                             :  :     +- Scan csv  (16)\n",
      "                                             :  +- Project (21)\n",
      "                                             :     +- Filter (20)\n",
      "                                             :        +- Scan csv  (19)\n",
      "                                             +- BroadcastExchange (25)\n",
      "                                                +- Project (24)\n",
      "                                                   +- Scan csv  (23)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [3]: [dr_no#639, lat#665, lon#666]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv]\n",
      "PushedFilters: [IsNotNull(lat), IsNotNull(lon), IsNotNull(dr_no)]\n",
      "ReadSchema: struct<dr_no:string,lat:float,lon:float>\n",
      "\n",
      "(2) Filter\n",
      "Input [3]: [dr_no#639, lat#665, lon#666]\n",
      "Condition : ((isnotnull(lat#665) AND isnotnull(lon#666)) AND isnotnull(dr_no#639))\n",
      "\n",
      "(3) Project\n",
      "Output [2]: [dr_no#639,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#779]\n",
      "Input [3]: [dr_no#639, lat#665, lon#666]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [3]: [dr_no#695, lat#721, lon#722]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv]\n",
      "PushedFilters: [IsNotNull(lat), IsNotNull(lon), IsNotNull(dr_no)]\n",
      "ReadSchema: struct<dr_no:string,lat:float,lon:float>\n",
      "\n",
      "(5) Filter\n",
      "Input [3]: [dr_no#695, lat#721, lon#722]\n",
      "Condition : ((isnotnull(lat#721) AND isnotnull(lon#722)) AND isnotnull(dr_no#695))\n",
      "\n",
      "(6) Project\n",
      "Output [2]: [dr_no#695,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#1211]\n",
      "Input [3]: [dr_no#695, lat#721, lon#722]\n",
      "\n",
      "(7) Union\n",
      "\n",
      "(8) Scan csv \n",
      "Output [3]: [X#827, Y#828, DIVISION#830]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv]\n",
      "ReadSchema: struct<X:double,Y:double,DIVISION:string>\n",
      "\n",
      "(9) Project\n",
      "Output [2]: [DIVISION#830 AS division#839,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS police_geom#846]\n",
      "Input [3]: [X#827, Y#828, DIVISION#830]\n",
      "\n",
      "(10) BroadcastExchange\n",
      "Input [2]: [division#839, police_geom#846]\n",
      "Arguments: IdentityBroadcastMode, [plan_id=2067]\n",
      "\n",
      "(11) BroadcastNestedLoopJoin\n",
      "Join type: Cross\n",
      "Join condition: isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Distance**  )\n",
      "\n",
      "(12) Project\n",
      "Output [3]: [dr_no#639, division#839,  **org.apache.spark.sql.sedona_sql.expressions.ST_Distance**   AS distance#884]\n",
      "Input [4]: [dr_no#639, crime_geom#779, division#839, police_geom#846]\n",
      "\n",
      "(13) Filter\n",
      "Input [3]: [dr_no#639, division#839, distance#884]\n",
      "Condition : true\n",
      "\n",
      "(14) Exchange\n",
      "Input [3]: [dr_no#639, division#839, distance#884]\n",
      "Arguments: hashpartitioning(dr_no#639, knownfloatingpointnormalized(normalizenanandzero(distance#884)), 1000), ENSURE_REQUIREMENTS, [plan_id=2172]\n",
      "\n",
      "(15) Sort\n",
      "Input [3]: [dr_no#639, division#839, distance#884]\n",
      "Arguments: [dr_no#639 ASC NULLS FIRST, knownfloatingpointnormalized(normalizenanandzero(distance#884)) ASC NULLS FIRST], false, 0\n",
      "\n",
      "(16) Scan csv \n",
      "Output [3]: [dr_no#929, lat#955, lon#956]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv]\n",
      "PushedFilters: [IsNotNull(lat), IsNotNull(lon), IsNotNull(dr_no)]\n",
      "ReadSchema: struct<dr_no:string,lat:float,lon:float>\n",
      "\n",
      "(17) Filter\n",
      "Input [3]: [dr_no#929, lat#955, lon#956]\n",
      "Condition : ((isnotnull(lat#955) AND isnotnull(lon#956)) AND isnotnull(dr_no#929))\n",
      "\n",
      "(18) Project\n",
      "Output [2]: [dr_no#929,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#779]\n",
      "Input [3]: [dr_no#929, lat#955, lon#956]\n",
      "\n",
      "(19) Scan csv \n",
      "Output [3]: [dr_no#957, lat#983, lon#984]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv]\n",
      "PushedFilters: [IsNotNull(lat), IsNotNull(lon), IsNotNull(dr_no)]\n",
      "ReadSchema: struct<dr_no:string,lat:float,lon:float>\n",
      "\n",
      "(20) Filter\n",
      "Input [3]: [dr_no#957, lat#983, lon#984]\n",
      "Condition : ((isnotnull(lat#983) AND isnotnull(lon#984)) AND isnotnull(dr_no#957))\n",
      "\n",
      "(21) Project\n",
      "Output [2]: [dr_no#957,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#1212]\n",
      "Input [3]: [dr_no#957, lat#983, lon#984]\n",
      "\n",
      "(22) Union\n",
      "\n",
      "(23) Scan csv \n",
      "Output [2]: [X#985, Y#986]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv]\n",
      "ReadSchema: struct<X:double,Y:double>\n",
      "\n",
      "(24) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS police_geom#846]\n",
      "Input [2]: [X#985, Y#986]\n",
      "\n",
      "(25) BroadcastExchange\n",
      "Input [1]: [police_geom#846]\n",
      "Arguments: IdentityBroadcastMode, [plan_id=2071]\n",
      "\n",
      "(26) BroadcastNestedLoopJoin\n",
      "Join type: Cross\n",
      "Join condition: None\n",
      "\n",
      "(27) Project\n",
      "Output [2]: [dr_no#929,  **org.apache.spark.sql.sedona_sql.expressions.ST_Distance**   AS distance#884]\n",
      "Input [3]: [dr_no#929, crime_geom#779, police_geom#846]\n",
      "\n",
      "(28) HashAggregate\n",
      "Input [2]: [dr_no#929, distance#884]\n",
      "Keys [1]: [dr_no#929]\n",
      "Functions [1]: [partial_min(distance#884)]\n",
      "Aggregate Attributes [1]: [min#1036]\n",
      "Results [2]: [dr_no#929, min#1037]\n",
      "\n",
      "(29) Exchange\n",
      "Input [2]: [dr_no#929, min#1037]\n",
      "Arguments: hashpartitioning(dr_no#929, 1000), ENSURE_REQUIREMENTS, [plan_id=2076]\n",
      "\n",
      "(30) HashAggregate\n",
      "Input [2]: [dr_no#929, min#1037]\n",
      "Keys [1]: [dr_no#929]\n",
      "Functions [1]: [min(distance#884)]\n",
      "Aggregate Attributes [1]: [min(distance#884)#925]\n",
      "Results [2]: [dr_no#929, min(distance#884)#925 AS min_distance#926]\n",
      "\n",
      "(31) Filter\n",
      "Input [2]: [dr_no#929, min_distance#926]\n",
      "Condition : isnotnull(min_distance#926)\n",
      "\n",
      "(32) Exchange\n",
      "Input [2]: [dr_no#929, min_distance#926]\n",
      "Arguments: hashpartitioning(dr_no#929, knownfloatingpointnormalized(normalizenanandzero(min_distance#926)), 1000), ENSURE_REQUIREMENTS, [plan_id=2082]\n",
      "\n",
      "(33) Sort\n",
      "Input [2]: [dr_no#929, min_distance#926]\n",
      "Arguments: [dr_no#929 ASC NULLS FIRST, knownfloatingpointnormalized(normalizenanandzero(min_distance#926)) ASC NULLS FIRST], false, 0\n",
      "\n",
      "(34) SortMergeJoin\n",
      "Left keys [2]: [dr_no#639, knownfloatingpointnormalized(normalizenanandzero(distance#884))]\n",
      "Right keys [2]: [dr_no#929, knownfloatingpointnormalized(normalizenanandzero(min_distance#926))]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(35) Project\n",
      "Output [3]: [dr_no#639, division#839, distance#884 AS distance_to_station#1004]\n",
      "Input [5]: [dr_no#639, division#839, distance#884, dr_no#929, min_distance#926]\n",
      "\n",
      "(36) HashAggregate\n",
      "Input [3]: [dr_no#639, division#839, distance_to_station#1004]\n",
      "Keys [1]: [division#839]\n",
      "Functions [2]: [partial_count(dr_no#639), partial_avg(distance_to_station#1004)]\n",
      "Aggregate Attributes [3]: [count#1214L, sum#1032, count#1033L]\n",
      "Results [4]: [division#839, count#1215L, sum#1034, count#1035L]\n",
      "\n",
      "(37) Exchange\n",
      "Input [4]: [division#839, count#1215L, sum#1034, count#1035L]\n",
      "Arguments: hashpartitioning(division#839, 1000), ENSURE_REQUIREMENTS, [plan_id=2177]\n",
      "\n",
      "(38) HashAggregate\n",
      "Input [4]: [division#839, count#1215L, sum#1034, count#1035L]\n",
      "Keys [1]: [division#839]\n",
      "Functions [2]: [count(dr_no#639), avg(distance_to_station#1004)]\n",
      "Aggregate Attributes [2]: [count(dr_no#639)#1011L, avg(distance_to_station#1004)#1013]\n",
      "Results [3]: [division#839, count(dr_no#639)#1011L AS crime_count#1012L, avg(distance_to_station#1004)#1013 AS avg_distance#1014]\n",
      "\n",
      "(39) Exchange\n",
      "Input [3]: [division#839, crime_count#1012L, avg_distance#1014]\n",
      "Arguments: rangepartitioning(crime_count#1012L DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=2179]\n",
      "\n",
      "(40) Sort\n",
      "Input [3]: [division#839, crime_count#1012L, avg_distance#1014]\n",
      "Arguments: [crime_count#1012L DESC NULLS LAST], true, 0\n",
      "\n",
      "(41) AdaptiveSparkPlan\n",
      "Output [3]: [division#839, crime_count#1012L, avg_distance#1014]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "Execution time for Query 4 (4 core, 2 executors, 8GB memory ): 48.9208 sec"
     ]
    }
   ],
   "source": [
    "from sedona.spark import SedonaContext\n",
    "from sedona.register.geo_registrator import SedonaRegistrator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "from sedona.sql.types import GeometryType\n",
    "from sedona.sql import ST_Point, ST_Distance\n",
    "from pyspark.sql.functions import min as sql_min\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "# -------------------------------------------------\n",
    "# 1. Spark + Sedona setup\n",
    "# -------------------------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query Nearest Police Station\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Load crimes\n",
    "# -------------------------------------------------\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"dr_no\", StringType()),\n",
    "    StructField(\"date_rptd\", StringType()),\n",
    "    StructField(\"date_occ\", StringType()),\n",
    "    StructField(\"time_occ\", StringType()),\n",
    "    StructField(\"area\", StringType()),\n",
    "    StructField(\"area_name\", StringType()),\n",
    "    StructField(\"rpt_dist_no\", StringType()),\n",
    "    StructField(\"part_1_2\", IntegerType()),\n",
    "    StructField(\"crm_cd\", StringType()),\n",
    "    StructField(\"crm_cd_desc\", StringType()),\n",
    "    StructField(\"mocodes\", StringType()),\n",
    "    StructField(\"vict_age\", StringType()),\n",
    "    StructField(\"vict_sex\", StringType()),\n",
    "    StructField(\"vict_descent\", StringType()),\n",
    "    StructField(\"premis_cd\", StringType()),\n",
    "    StructField(\"premis_desc\", StringType()),\n",
    "    StructField(\"weapon_used_cd\", StringType()),\n",
    "    StructField(\"weapon_desc\", StringType()),\n",
    "    StructField(\"status\", StringType()),\n",
    "    StructField(\"status_desc\", StringType()),\n",
    "    StructField(\"crm_cd_1\", StringType()),\n",
    "    StructField(\"crm_cd_2\", StringType()),\n",
    "    StructField(\"crm_cd_3\", StringType()),\n",
    "    StructField(\"crm_cd_4\", StringType()),\n",
    "    StructField(\"location\", StringType()),\n",
    "    StructField(\"cross_street\", StringType()),\n",
    "    StructField(\"lat\", FloatType()),\n",
    "    StructField(\"lon\", FloatType()),\n",
    "])\n",
    "\n",
    "crimes_2010_2019_df = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\",\n",
    "    header=False, schema=crimes_schema\n",
    ")\n",
    "\n",
    "crimes_2020_2025_df = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "    header=False, schema=crimes_schema\n",
    ")\n",
    "\n",
    "# Combine datasets\n",
    "crimes_total_df = crimes_2010_2019_df.union(crimes_2020_2025_df)\n",
    "\n",
    "# Filter records with coordinates\n",
    "crimes_points = crimes_total_df \\\n",
    "    .filter(col(\"lat\").isNotNull() & col(\"lon\").isNotNull()) \\\n",
    "    .withColumn(\"crime_geom\", ST_Point(col(\"lon\"), col(\"lat\")))\n",
    "\n",
    "\n",
    "police_df = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv\",\n",
    "    header=True, inferSchema=True\n",
    ").select(\n",
    "    col(\"DIVISION\").alias(\"division\"),\n",
    "    col(\"X\").alias(\"lon\"),\n",
    "    col(\"Y\").alias(\"lat\")\n",
    ")\n",
    "\n",
    "# Convert to geometry\n",
    "police_points = police_df.withColumn(\n",
    "    \"police_geom\",\n",
    "    ST_Point(col(\"lon\"), col(\"lat\"))\n",
    ")\n",
    "\n",
    "\n",
    "joined = crimes_points.crossJoin(police_points) \\\n",
    "    .withColumn(\"distance\", ST_Distance(col(\"crime_geom\"), col(\"police_geom\"))) \\\n",
    "    .select(\n",
    "        col(\"dr_no\"),\n",
    "        col(\"division\"),\n",
    "        col(\"distance\")\n",
    "    )\n",
    "\n",
    "j = joined.alias(\"j\")\n",
    "\n",
    "# 1. Minimum distance per crime\n",
    "min_dist = j.groupBy(col(\"j.dr_no\")).agg(\n",
    "    sql_min(\"j.distance\").alias(\"min_distance\")\n",
    ").alias(\"m\")\n",
    "\n",
    "# 2. Join χωρίς ambiguous dr_no\n",
    "nearest = j.join(\n",
    "    min_dist,\n",
    "    (col(\"j.dr_no\") == col(\"m.dr_no\")) &\n",
    "    (col(\"j.distance\") == col(\"m.min_distance\")),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"j.dr_no\").alias(\"dr_no\"),\n",
    "    col(\"j.division\").alias(\"division\"),\n",
    "    col(\"j.distance\").alias(\"distance_to_station\")\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6. Compute crime_count + avg_distance per division\n",
    "# -------------------------------------------------\n",
    "from pyspark.sql.functions import avg, count\n",
    "\n",
    "division_stats = nearest.groupBy(\"division\").agg(\n",
    "    count(\"dr_no\").alias(\"crime_count\"),\n",
    "    avg(\"distance_to_station\").alias(\"avg_distance\")\n",
    ").orderBy(col(\"crime_count\").desc())\n",
    "\n",
    "division_stats.show(truncate=False)\n",
    "division_stats.explain(\"formatted\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution time for Query 4 (4 core, 2 executors, 8GB memory ): {:.4f} sec\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78b16aa-5d78-4e46-bcd6-8088ebb76257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
